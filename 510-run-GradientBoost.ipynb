{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "from glob import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tree(fname):\n",
    "    '''\n",
    "    fname: filename\n",
    "    return: {date: [title, text]} after parsing the xml file\n",
    "    '''\n",
    "    tree = etree.parse(fname)\n",
    "    root = tree.getroot()\n",
    "    subject = []\n",
    "    for idx, wrt in enumerate(root.findall(\"WRITING\")):\n",
    "        title = wrt.find('TITLE').text\n",
    "        if title != None:\n",
    "            title = \" \".join(title.replace('\\n',' ').strip().split())\n",
    "        else:\n",
    "            title = \"\"\n",
    "        \n",
    "        date = wrt.find('DATE').text\n",
    "        if date != None:\n",
    "            date = \" \".join(date.replace('\\n',' ').strip().split())\n",
    "        else:\n",
    "            date = \"\"\n",
    "        \n",
    "        text = wrt.find('TEXT').text\n",
    "        if text != None:\n",
    "            text = \" \".join(text.replace('\\n',' ').strip().split())\n",
    "        else:\n",
    "            text = \"\"\n",
    "        \n",
    "        if date==\"\" and title==\"\" and text==\"\":\n",
    "            continue\n",
    "        \n",
    "        subject.append([date, title, text])\n",
    "        \n",
    "\n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "from glob import glob\n",
    "fpaths = sorted(glob(\"train/data/*.xml\"), key=lambda x: int(x[18:-4]))\n",
    "for fp in fpaths:\n",
    "#     print(fp[18:-4])\n",
    "    corpus.append(parse_tree(fp))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = []\n",
    "from glob import glob\n",
    "# print(glob(\"test/DATA/*.xml\")[0])\n",
    "fpaths = sorted(glob(\"test/DATA/*.xml\"), key=lambda x: int(x[17:-4]))\n",
    "for fp in fpaths:\n",
    "#     print(fp)\n",
    "    testset.append(parse_tree(fp))\n",
    "\n",
    "    \n",
    "file = open(\"test/T1_erisk_golden_truth.txt\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "gt_list_test = []\n",
    "for line in lines:\n",
    "    gt = line.split(\" \")\n",
    "    gt[1] = int(gt[1])\n",
    "    gt_list_test.append(gt)\n",
    "#     print(gt)\n",
    "gt_list_test = sorted(gt_list_test, key=lambda x: int(x[0][7:]))\n",
    "Y_test = [elem[1] for elem in gt_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.json\", 'w+') as f:\n",
    "    json.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train/golden_truth.txt\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "gt_list = []\n",
    "for line in lines:\n",
    "    gt = line.split(\" \")\n",
    "    gt[1] = int(gt[1])\n",
    "    gt_list.append(gt)\n",
    "#     print(gt)\n",
    "\n",
    "gt_list = sorted(gt_list, key=lambda x: int(x[0][7:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = [elem[1] for elem in gt_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12058823529411765"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt)/len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_test = copy.deepcopy(testset)\n",
    "for subj in X_test:\n",
    "    for doc in subj:\n",
    "        del doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test, Y_test, model, c_fp=0.12, c_fn=1, c_tp=1, o = 0):\n",
    "    '''\n",
    "    X_test: [[title, text]]; size = total number of entries in all subjects; nested array of subject and entries\n",
    "    Y_test: [0/1]; size = # of subjects\n",
    "    model: \n",
    "    return: sum of average score for each subject\n",
    "    '''\n",
    "    erde = []\n",
    "    for (subject, y) in tqdm(zip(X_test, Y_test), total = len(Y_test)):\n",
    "        score = 0\n",
    "        for idx, doc in enumerate(subject):\n",
    "            token = [doc[0]+' '+doc[1]]\n",
    "            pred = model.predict(token)\n",
    "            if y == 0: # negative\n",
    "                if pred == 0:\n",
    "                    score += c_fp\n",
    "                else:\n",
    "                    score += c_fn\n",
    "            else: # positive\n",
    "                if pred == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    score += c_tp * (1 - 1/(1+np.exp(idx - o)))\n",
    "#         print(score)\n",
    "        erde.append(score)\n",
    "    return erde\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newline-seperated document with title + text\n",
    "new_corpus = []\n",
    "for subj in corpus:\n",
    "    tmp = \"\"\n",
    "    for doc in subj:\n",
    "#         print(doc)\n",
    "        tmp += (doc[1]+\" \"+doc[2]).replace(\"\\'\",\"\")\n",
    "    new_corpus.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feature = []\n",
    "for subj in testset:\n",
    "    tmp = \"\"\n",
    "    for doc in subj:\n",
    "#         print(doc)\n",
    "        tmp += (doc[1]+\" \"+doc[2]).replace(\"\\'\",\"\")\n",
    "    X_test_feature.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [len(subj) for subj in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=\"english\", ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf = text_clf.fit(new_corpus, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7541371158392435"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_nb = text_clf.predict(X_test_feature)\n",
    "np.mean(predicted_nb == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 51/423 [04:42<48:52,  7.88s/it]  "
     ]
    }
   ],
   "source": [
    "erde_nb = evaluate(X_test, Y_test, text_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7754137115839244"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words=\"english\", ngram_range=(1,2))),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42)),\n",
    "])\n",
    "_ = text_clf_svm.fit(new_corpus, gt)\n",
    "predicted_svm = text_clf_svm.predict(X_test_feature)\n",
    "np.mean(predicted_svm == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 52/423 [03:09<29:37,  4.79s/it]"
     ]
    }
   ],
   "source": [
    "erde_svm = evaluate(X_test, Y_test, text_clf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8250591016548463"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf-svm__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf_svm, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(new_corpus, gt)\n",
    "predicted_grid = gs_clf.predict(X_test_feature)\n",
    "np.mean(predicted_grid == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_gs = evaluate(X_test, Y_test, gs_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966903073286052"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "text_clf_gbbost = Pipeline([('vect', CountVectorizer(stop_words=\"english\", ngram_range=(1,2))),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-gboost', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0))\n",
    "])\n",
    "_ = text_clf_gbbost.fit(new_corpus, gt)\n",
    "predicted_xgboost = text_clf_gbbost.predict(X_test_feature)\n",
    "np.mean(predicted_xgboost == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 119/423 [24:15<13:59,  2.76s/it]  <ipython-input-11-c8fdd5f46477>:23: RuntimeWarning: overflow encountered in exp\n",
      "  score += c_tp * (1 - 1/(1+np.exp(idx - o)))\n",
      "100%|██████████| 423/423 [1:37:31<00:00, 13.83s/it]  \n"
     ]
    }
   ],
   "source": [
    "erde_gb = evaluate(X_test, Y_test, text_clf_gbbost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_and_save(erde_gs, fpaths, output_name):\n",
    "    scores = np.array([v/nums[idx] for idx, v in enumerate(erde_gs)])\n",
    "    ranking_result = []\n",
    "    for value, file in zip(scores, fpaths):\n",
    "        ranking_result.append((value, file))\n",
    "    ranking_result.sort(reverse=True)\n",
    "    with open(output_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for v, p in ranking_result:\n",
    "            f.write(f\"{v} {p}\\n\")\n",
    "rank_and_save(erde_gb, fpaths, \"GradientBoost.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652482269503546"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "text_clf_ada = Pipeline([\n",
    "                      ('vect', CountVectorizer(stop_words=\"english\", ngram_range=(1,2))),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-ada', AdaBoostClassifier(n_estimators=100)),\n",
    "])\n",
    "_ = text_clf_ada.fit(new_corpus, gt)\n",
    "predicted_adaboost = text_clf_ada.predict(X_test_feature)\n",
    "np.mean(predicted_adaboost == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_ada = evaluate(X_test, Y_test, text_clf_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
